{
  "cells": [
    {
      "cell_type": "raw",
      "id": "5d691a73",
      "metadata": {
        "id": "5d691a73"
      },
      "source": [
        "---\n",
        "# Please don't change the text below, it's for formatting the documentation.\n",
        "title: \"Fully Fine-tune a Small Language Model with Hugging Face Tutorial\"\n",
        "description: \"Learn how to fully fine-tune a Small Language Model on a custom dataset with Hugging Face Transformers.\"\n",
        "image: \"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/00-project-food-not-food-overview.png\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    page-layout: full\n",
        "jupyter: python3\n",
        "number-sections: true\n",
        "toc: true\n",
        "toc-depth: 3\n",
        "toc-expand: 2 # expand toc to multiple levels\n",
        "code-block-border-left: true\n",
        "code-block-bg: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c02374e",
      "metadata": {
        "id": "0c02374e"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_llm_full_fine_tune_tutorial.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "> **Note:** If you're running in Google Colab, make sure to enable GPU usage by going to Runtime -> Change runtime type -> select GPU.\n",
        "\n",
        "[Source Code](https://github.com/mrdbourke/learn-huggingface/blob/main/notebooks/hugging_face_llm_full_fine_tune_tutorial.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Why fine-tune a Small Language Model (SLM)?\n",
        "\n",
        "Because we want small model for a specific task (e.g. don't to pay API credits or leak our data online).\n",
        "\n",
        "If we have our own model... we can run it anywhere and everywhere we like.\n",
        "\n",
        "## How to fine-tune an LLM model\n",
        "\n",
        "There are several ways to fine-tune an LLM including reinforcement learning (RL) and Supervised Fine-tuning (SFT).\n",
        "\n",
        "We are going to do SFT because it's the most straightforward.\n",
        "\n",
        "Basically SFT = give samples of inputs and outputs and the model learns to map a given input to a given output.\n",
        "\n",
        "For example if our goal was to extract names:\n",
        "\n",
        "- Input: Hello my name is Daniel\n",
        "- Output: Daniel\n",
        "\n",
        "Note:\n",
        "\n",
        "* Our inputs can be *any* kind of input string.\n",
        "* Our outputs will be fine-tuned to conform to a structured data pattern.\n",
        "\n",
        "::: {.callout-tip}\n",
        "## LLM Fine-tuning Mindset\n",
        "\n",
        "In LLM world, data inputs are tokens and data outputs are tokens.\n",
        "\n",
        "A token is a numerical representation of some kind of data.\n",
        "\n",
        "Computers like numbers (not images, text, videos, etc).\n",
        "\n",
        "Everything must be turned into numbers.\n",
        "\n",
        "And data = a very broad term.\n",
        "\n",
        "It could be text, images, video (series of images), audio, DNA sequences, Excel spreadsheets, you name it.\n",
        "\n",
        "The goal of the LLM is to be given an input sequence of tokens and then predict the following tokens.\n",
        "\n",
        "So with this mindset, you can think of any problem as **tokens in, tokens out**.\n",
        "\n",
        "Ask yourself: *What tokens do I want to put in and what tokens do I want my model to return?*\n",
        "\n",
        "In our case, we want to put in almost any string input. And we want to get back structured information specifically related to food and drinks.\n",
        "\n",
        "This a very specific use case, however, the beauty of LLMs being so general is that you can apply this **tokens in, tokens out** mindset to almost anything.\n",
        "\n",
        "If you've got an existing dataset (no problem if you don't, you can create one, let me know if you'd like a guide on this), chances are, you can fine-tune an LLM to do pretty well on it.\n",
        ":::\n",
        "\n",
        "## What we're cooking\n",
        "\n",
        "We're going to build a SLM (Small Language Model) to extract food and drink items from text.\n",
        "\n",
        "Why?\n",
        "\n",
        "If we needed to go over a large dataset of image captions and filter them for food items (we could then use these filtered captions for a food app).\n",
        "\n",
        "TK image - example of what we're doing.\n",
        "\n",
        "## Ingredients\n",
        "\n",
        "1. Model ([Gemma-3-270M](https://huggingface.co/google/gemma-3-270m-it))\n",
        "2. [Dataset](https://huggingface.co/datasets/mrdbourke/FoodExtract-1k) (a pre-baked dataset to extract foods and drinks from text)\n",
        "3. Training code (Hugging Face Transformers + TRL)\n",
        "4. Eval code\n",
        "5. Demo\n",
        "\n",
        "## Method\n",
        "\n",
        "1. Download model - [Hugging Face `transformers`](https://huggingface.co/docs/transformers/index)\n",
        "2. Download dataset - [Hugging Face `datasets`](https://huggingface.co/docs/datasets/en/index)\n",
        "3. Inspect dataset - Hugging Face `datasets`\n",
        "4. Train model on dataset - [Hugging Face `trl`](https://huggingface.co/docs/trl/en/index) (TRL = Transformers Reinforcement Learning)\n",
        "5. Eval model - basically just look at a bunch of samples\n",
        "6. Create an interactive demo - Hugging Face `gradio`\n",
        "7. Bonus: Make the demo public so other people can use it - Hugging Face Spaces\n",
        "\n",
        "## Fine-tuning LLM vs RAG\n",
        "\n",
        "* Fine-tuning = To do a very specific task, e.g. structured data extraction.\n",
        "    * An example would be you're an insurance company who gets 10,000 emails a day and you want to extract structured data directly from these emails to JSON.\n",
        "* RAG = You want to inject custom knowledge into an LLM.\n",
        "    * An example would be you're an insurance company wanting to send *automatic* responses to people but you want the responses to include information from your own docs.\n",
        "\n",
        "## Why fine-tune your own model?\n",
        "\n",
        "1. Own the model, can run on own hardware\n",
        "2. Our task is simple enough to just use a small language model\n",
        "3. No API calls needed\n",
        "4. Can run in batch mode to get much faster inference than API calls\n",
        "5. Model by default wasn't very good at our task but now since fine-tuning, is *very good*\n",
        "\n",
        "## Definitions\n",
        "\n",
        "Some quick definitions of what we're doing.\n",
        "\n",
        "* **Full fine-tuning** - All weights on the model are updated. Often takes longer and requires larger hardware capacity, however, if your model is small enough (e.g. 270M parameter or less), you can often do full fine-tuning.\n",
        "* **LORA fine-tuning** (also known as partial fine-tuning) - [Low Rank Adaptation](https://arxiv.org/abs/2106.09685) or training a small adapter to attach to your original model. Requires significantly less resources but [can perform on par with full fine-tuning](https://thinkingmachines.ai/blog/lora/).\n",
        "* **SLM (Small Language Model)** - A subjective definition but to me a Small Language Model is a model with under 1B parameters, with added bonus for being under 500M parameters. Less parameters generally means less performance. However, when you have a specific task, SLMs often shine because they can be tailored for that specific task. If your task is \"I want to create a chatbot capable of *anything*\", you'll generally want the biggest model you can reasonably serve. If your task is \"I want to extract some structured data from raw text inputs\", you'll probably be surprised how well a SLM can perform."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "692c2190",
      "metadata": {
        "id": "692c2190"
      },
      "source": [
        "## Import dependencies\n",
        "\n",
        "> **Note:** If you're in Google Colab, you may have to install `trl`, `accelerate` and `gradio`.\n",
        "\n",
        "For Google Colab:\n",
        "\n",
        "```python\n",
        "!pip install trl accelerate gradio\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "421530e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "421530e1",
        "outputId": "d2ccd86e-4b50-486f-913b-b157ab16402e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.26.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.21)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install trl accelerate gradio\n",
        "\n",
        "import transformers\n",
        "import trl # trl = Transformers Reinforcement Learning -> https://github.com/huggingface/trl\n",
        "import datasets\n",
        "import accelerate\n",
        "\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "629dcca3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629dcca3",
        "outputId": "4c3be436-c8cf-4ebf-e6b1-77dae9e2b645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "Total Memory:     15828.32 MB | 15.83 GB\n",
            "Allocated Memory: 0.00 MB | 0.00 GB\n",
            "Reserved Memory:  0.00 MB | 0.00 GB\n",
            "Free Memory:      15828.32 MB | 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "# Check the amount of GPU memory available (we need at least ~16GB)\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    gpu_name = torch.cuda.get_device_name(device)\n",
        "\n",
        "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
        "    allocated_memory = torch.cuda.memory_allocated(device)\n",
        "    reserved_memory = torch.cuda.memory_reserved(device)\n",
        "    free_memory = total_memory - reserved_memory\n",
        "\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"Total Memory:     {total_memory / 1e6:.2f} MB | {total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Allocated Memory: {allocated_memory / 1e6:.2f} MB | {allocated_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Reserved Memory:  {reserved_memory / 1e6:.2f} MB | {reserved_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Free Memory:      {free_memory / 1e6:.2f} MB | {free_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"No CUDA GPU available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492095ab",
      "metadata": {
        "id": "492095ab"
      },
      "source": [
        "## Setup Base Model\n",
        "\n",
        "The base model we'll be using is [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m-it/tree/main) from Google.\n",
        "\n",
        "It's the same architecture style as larger LLMs such as Gemini but at a *much* smaller scale.\n",
        "\n",
        "This is why we refer to it as a \"Small Language Model\" or SLM.\n",
        "\n",
        "We can load our model using `transformers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4de92467",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "4de92467",
        "outputId": "2d362124-e82b-4067-fcd7-0336ea641be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-3-270m-it.\n401 Client Error. (Request ID: Root=1-6967c989-1221388f0b036a42085e24d7;275c58e2-774a-4868-939d-739864689434)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-270m-it/resolve/main/config.json.\nAccess to model google/gemma-3-270m-it is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-3-270m-it/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1008\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1544\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1461\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6967c989-1221388f0b036a42085e24d7;275c58e2-774a-4868-939d-739864689434)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-270m-it/resolve/main/config.json.\nAccess to model google/gemma-3-270m-it is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-739913988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/gemma-3-270m-it\"\u001b[0m \u001b[0;31m# note: \"it\" stands for \"instruction tuned\" which means the model has been tuned for following instructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    722\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-3-270m-it.\n401 Client Error. (Request ID: Root=1-6967c989-1221388f0b036a42085e24d7;275c58e2-774a-4868-939d-739864689434)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-3-270m-it/resolve/main/config.json.\nAccess to model google/gemma-3-270m-it is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-270m-it\" # note: \"it\" stands for \"instruction tuned\" which means the model has been tuned for following instructions\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\", # put the model on the GPU\n",
        "    attn_implementation=\"eager\" # could use flash_attention_2 but ran into issues... so stick with Eager for now\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae5a6b9",
      "metadata": {
        "id": "8ae5a6b9"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"[INFO] Model on device: {model.device}\")\n",
        "print(f\"[INFO] Model using dtype: {model.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ceee35",
      "metadata": {
        "id": "37ceee35"
      },
      "source": [
        "Our model requires numbers (tokens) as input.\n",
        "\n",
        "We can turn strings into tokens via a tokenizer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d187d0",
      "metadata": {
        "id": "e8d187d0"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"Hello my name is Daniel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf6296a",
      "metadata": {
        "id": "5cf6296a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "outputs = model(torch.tensor(tokenizer(\"Hello my name is Daniel\")[\"input_ids\"]).unsqueeze(0).to(\"cuda\"))\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "629cb237",
      "metadata": {
        "id": "629cb237"
      },
      "source": [
        "## Get dataset\n",
        "\n",
        "Our dataset is located here: [https://huggingface.co/datasets/mrdbourke/FoodExtract-1k](https://huggingface.co/datasets/mrdbourke/FoodExtract-1k).\n",
        "\n",
        "It was created from image captions + random strings and then using `gpt-oss-120b` (a powerful open-source LLM) to do synthetic labelling.\n",
        "\n",
        "For more on the dataset you can read the [README.md](https://huggingface.co/datasets/mrdbourke/FoodExtract-1k) file explaining it.\n",
        "\n",
        "The main thing we are concerned about is that we want the input to our model to be the `\"sequence\"` column and the output to be the `\"gpt-oss-120b-label-condensed\"` column.\n",
        "\n",
        "We'll explore these below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5721a3b1",
      "metadata": {
        "id": "5721a3b1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mrdbourke/FoodExtract-1k\")\n",
        "\n",
        "print(f\"[INFO] Number of samples in the dataset: {len(dataset['train'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa6954c",
      "metadata": {
        "id": "3fa6954c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "def get_random_idx(dataset):\n",
        "    \"\"\"Returns a random integer index based on the number of samples in the dataset.\"\"\"\n",
        "    random_idx = random.randint(0, len(dataset)-1)\n",
        "    return random_idx\n",
        "\n",
        "\n",
        "random_idx = get_random_idx(dataset[\"train\"])\n",
        "random_sample = dataset[\"train\"][random_idx]\n",
        "\n",
        "example_input = random_sample[\"sequence\"]\n",
        "example_output = random_sample[\"gpt-oss-120b-label\"]\n",
        "example_output_condensed = random_sample[\"gpt-oss-120b-label-condensed\"]\n",
        "\n",
        "print(f\"[INFO] Input:\\n{example_input}\")\n",
        "print()\n",
        "print(f\"[INFO] Example structured output (what we want our model to learn to predict):\")\n",
        "print(eval(example_output))\n",
        "print()\n",
        "print(f\"[INFO] Example output condensed (we'll train our model to predict the condensed output since it uses less tokens than JSON):\")\n",
        "print(example_output_condensed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d285595-bd57-4655-900d-d0b448d11056",
      "metadata": {
        "id": "4d285595-bd57-4655-900d-d0b448d11056"
      },
      "source": [
        "Because we'd like to use our model to potentially filter a large corpus of data, we get it assign various tags to the text as well.\n",
        "\n",
        "These are as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4195702-aa43-42c6-9006-c43d1c02696e",
      "metadata": {
        "id": "a4195702-aa43-42c6-9006-c43d1c02696e"
      },
      "outputs": [],
      "source": [
        "# Our fine-tuned model will assign tags to text so we can easily filter them by type in the future\n",
        "tags_dict = {'np': 'nutrition_panel',\n",
        " 'il': 'ingredient list',\n",
        " 'me': 'menu',\n",
        " 're': 'recipe',\n",
        " 'fi': 'food_items',\n",
        " 'di': 'drink_items',\n",
        " 'fa': 'food_advertistment',\n",
        " 'fp': 'food_packaging'}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e80cff0",
      "metadata": {
        "id": "9e80cff0"
      },
      "source": [
        "### Format the dataset into LLM-style inputs/outputs\n",
        "\n",
        "Right now we have examples of string-based inputs and structured outputs.\n",
        "\n",
        "However, our LLMs generally want things in the format of:\n",
        "\n",
        "```\n",
        "{\"user\": \"Hello my name is Daniel\",\n",
        "\"system\": \"Hi Daniel, I'm an LLM\"}\n",
        "```\n",
        "\n",
        "In other words, they want structure around the intputs and outputs rather than just raw information.\n",
        "\n",
        "> **Resource:** See the dataset formats and types in the TRL docs: https://huggingface.co/docs/trl/en/dataset_formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d909b36",
      "metadata": {
        "id": "6d909b36"
      },
      "outputs": [],
      "source": [
        "random_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16769fa3",
      "metadata": {
        "id": "16769fa3"
      },
      "outputs": [],
      "source": [
        "def sample_to_conversation(sample):\n",
        "    \"\"\"Helper function to convert an input sample to conversation style.\"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": sample[\"sequence\"]}, # Load the sequence from the dataset\n",
        "            {\"role\": \"system\", \"content\": sample[\"gpt-oss-120b-label-condensed\"]} # Load the gpt-oss-120b generated label\n",
        "        ]\n",
        "    }\n",
        "\n",
        "sample_to_conversation(random_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cca9d2",
      "metadata": {
        "id": "09cca9d2"
      },
      "outputs": [],
      "source": [
        "# Map our sample_to_conversation function to dataset\n",
        "dataset = dataset.map(sample_to_conversation,\n",
        "                      batched=False)\n",
        "\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a46d77-b93d-4f52-8ec0-7363900f44f3",
      "metadata": {
        "id": "44a46d77-b93d-4f52-8ec0-7363900f44f3"
      },
      "source": [
        "Notice how we now have a `\"messages\"` key in our dataset samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f027190f",
      "metadata": {
        "id": "f027190f"
      },
      "outputs": [],
      "source": [
        "# Create a train/test split\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2,\n",
        "                                            shuffle=False,\n",
        "                                            seed=42)\n",
        "\n",
        "# Number #1 rule in machine learning\n",
        "# Always train on the train set and test on the test set\n",
        "# This gives us an indication of how our model will perform in the real world\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84196f5b",
      "metadata": {
        "id": "84196f5b"
      },
      "source": [
        "### Try the model with a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2738c3",
      "metadata": {
        "id": "ea2738c3"
      },
      "outputs": [],
      "source": [
        "easy_sample = {\"role\": \"user\",\n",
        "               \"content\": \"Hi my name is Daniel\"}\n",
        "\n",
        "def create_easy_sample(input):\n",
        "    template = {\"role\": \"user\", \"content\": input}\n",
        "    return template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9871a6",
      "metadata": {
        "id": "1a9871a6"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load model and use it as a pipeline\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer)\n",
        "\n",
        "input_text = \"Hi my name is Daniel. Please reply to me with a machine learning poem.\"\n",
        "easy_sample = create_easy_sample(input=input_text)\n",
        "input_prompt = pipe.tokenizer.apply_chat_template([easy_sample], # pipeline tokenizer wants a list of inputs\n",
        "                                                  tokenize=False,\n",
        "                                                  add_generation_prompt=True)\n",
        "\n",
        "print(f\"[INFO] This is the input prompt: {input_prompt}\")\n",
        "\n",
        "default_outputs = pipe(input_prompt,\n",
        "                       max_new_tokens=512,\n",
        "                       disable_compile=True)\n",
        "\n",
        "print(f\"[INFO] Input:\\n{input_text}\")\n",
        "print()\n",
        "print(f\"[INFO] Output from {MODEL_NAME}:\")\n",
        "print()\n",
        "print(default_outputs[0][\"generated_text\"][len(input_prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac4c124e",
      "metadata": {
        "id": "ac4c124e"
      },
      "source": [
        "Example machine learning poem generated by Gemma 3 270M (not too bad):\n",
        "\n",
        "```\n",
        "Okay, Daniel, here's a machine learning poem. I've tried to capture a feeling of wonder and a bit of mystery.\n",
        "\n",
        "The algorithm learns,\n",
        "A silent, tireless quest.\n",
        "Through data streams, it flows,\n",
        "A symphony of thought.\n",
        "Each point a new layer,\n",
        "A learning bloom,\n",
        "A future bright and clear.\n",
        "\n",
        "It analyzes the data,\n",
        "No single clue it knows.\n",
        "It weaves a pattern true,\n",
        "A story in the hue.\n",
        "The world unfolds anew,\n",
        "With subtle, complex view.\n",
        "\n",
        "It's not just numbers,\n",
        "But feeling, a soul.\n",
        "A tapestry of grace,\n",
        "A hopeful, vibrant space.\n",
        "A learning, growing deep,\n",
        "Secrets it will keep.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1536cc33",
      "metadata": {
        "id": "1536cc33"
      },
      "source": [
        "### Try the model on one of our sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39394d8b",
      "metadata": {
        "id": "39394d8b"
      },
      "outputs": [],
      "source": [
        "# Get a random sample\n",
        "random_idx = get_random_idx(dataset[\"train\"])\n",
        "random_train_sample = dataset[\"train\"][random_idx]\n",
        "\n",
        "# Apply the chat template\n",
        "input_prompt = pipe.tokenizer.apply_chat_template(conversation=random_train_sample[\"messages\"][:1],\n",
        "                                                  tokenize=False,\n",
        "                                                  add_generation_prompt=True)\n",
        "\n",
        "# Let's run the default model on our input\n",
        "default_outputs = pipe(text_inputs=input_prompt, max_new_tokens=256)\n",
        "\n",
        "# View and compare the outputs\n",
        "print(f\"[INFO] Input:\\n{input_prompt}\\n\")\n",
        "print(f\"[INFO] Output:\\n{default_outputs[0]['generated_text'][len(input_prompt):]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f4775f-ec4b-4e1d-875c-5e33213bde71",
      "metadata": {
        "id": "f9f4775f-ec4b-4e1d-875c-5e33213bde71"
      },
      "source": [
        "By default the model produces a fairly generic response.\n",
        "\n",
        "This is expected and good.\n",
        "\n",
        "It means the model has a good baseline understanding of language.\n",
        "\n",
        "If it responded with pure garbage, we might have an uphill battle.\n",
        "\n",
        "However, this response type is not what we want. We want our model to respond with **structured data** based on the input.\n",
        "\n",
        "Good news is we can adjust the patterns in our model to do just that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b70eec2",
      "metadata": {
        "id": "7b70eec2"
      },
      "source": [
        "### Let's try to prompt the model\n",
        "\n",
        "We want a model to extract food and drink items from text.\n",
        "\n",
        "By default the model will just reply to any text input with a generic response.\n",
        "\n",
        "However, we can try and get our ideal outputs via prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f93224",
      "metadata": {
        "id": "a1f93224"
      },
      "outputs": [],
      "source": [
        "prompt_instruction = \"\"\"Given the following target input text from an image caption, please extract the food and drink items to a list.\n",
        "If there are no food or drink items, return an empty list.\n",
        "\n",
        "Return in the following format:\n",
        "food_items: [item_1, item_2, item_3]\n",
        "drink_items: [item_4, item_5]\n",
        "\n",
        "For example:\n",
        "Input text: Hello my name is Daniel.\n",
        "Output:\n",
        "food_items: []\n",
        "drink_items: []\n",
        "\n",
        "Example 2:\n",
        "Input text: A plate of rice cakes, salmon, cottage cheese and small cherry tomatoes with a cup of tea.\n",
        "Output:\n",
        "food_items: ['rice cakes', 'salmon', 'cottage cheese', 'cherry tomatoes']\n",
        "drink_items: ['cup of tea']\n",
        "\n",
        "Return only the formatted output and nothing else.\n",
        "\n",
        "Target input text: <targ_input_text>\"\"\"\n",
        "\n",
        "def update_input_message_content(input):\n",
        "    original_content = input[\"messages\"][:1][0][\"content\"]\n",
        "    new_content = prompt_instruction.replace(\"<targ_input_text>\", original_content)\n",
        "\n",
        "    new_input = [{\"content\": new_content,\n",
        "                  \"role\": \"user\"}]\n",
        "\n",
        "    return new_input\n",
        "\n",
        "print(f'[INFO] Original content:\\n{random_train_sample[\"messages\"][:1][0][\"content\"]}')\n",
        "print()\n",
        "print(f'[INFO] New content with instructions in prompt:')\n",
        "print(update_input_message_content(input=random_train_sample)[0][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ed0f84",
      "metadata": {
        "id": "22ed0f84"
      },
      "outputs": [],
      "source": [
        "# Apply the chat template\n",
        "updated_input_prompt = update_input_message_content(input=random_train_sample)\n",
        "\n",
        "input_prompt = pipe.tokenizer.apply_chat_template(conversation=updated_input_prompt,\n",
        "                                                  tokenize=False,\n",
        "                                                  add_generation_prompt=True)\n",
        "\n",
        "# Let's run the default model on our input\n",
        "default_outputs = pipe(text_inputs=input_prompt,\n",
        "                       max_new_tokens=256)\n",
        "\n",
        "# View and compare the outputs\n",
        "print(f\"[INFO] Input:\\n{input_prompt}\\n\")\n",
        "print(f\"[INFO] Output:\\n{default_outputs[0]['generated_text'][len(input_prompt):]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2b1392",
      "metadata": {
        "id": "ff2b1392"
      },
      "outputs": [],
      "source": [
        "# This is our input\n",
        "print(random_train_sample[\"messages\"][0][\"content\"])\n",
        "print()\n",
        "\n",
        "# This is our ideal output:\n",
        "print(random_train_sample[\"messages\"][1][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe2e159",
      "metadata": {
        "id": "3fe2e159"
      },
      "source": [
        "Okay looks like our small LLM doesn't do what we want it to do... it starts to reply with Python text or unreliably extracts foods and drinks from text in a non-uniform format.\n",
        "\n",
        "No matter, we can fine-tune it so it does our specific task!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6784ab",
      "metadata": {
        "id": "bf6784ab"
      },
      "source": [
        "## Fine-tuning our model\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Setup SFTConfig (Supervised Fine-tuning Config) - https://huggingface.co/docs/trl/v0.26.2/en/sft_trainer#trl.SFTConfig\n",
        "2. Use SFTTrainer to train our model on our supervised samples (from our dataset above) - https://huggingface.co/docs/trl/v0.26.2/en/sft_trainer#trl.SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f29e96",
      "metadata": {
        "id": "c5f29e96"
      },
      "outputs": [],
      "source": [
        "# Setting up our SFTConfig\n",
        "from trl import SFTConfig\n",
        "\n",
        "torch_dtype = model.dtype\n",
        "\n",
        "CHECKPOINT_DIR_NAME = \"./checkpoint_models\"\n",
        "BASE_LEARNING_RATE = 5e-5\n",
        "\n",
        "print(f\"[INFO] Using dtype: {torch_dtype}\")\n",
        "print(f\"[INFO] Using learning rate: {BASE_LEARNING_RATE}\")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=CHECKPOINT_DIR_NAME,\n",
        "    max_length=512,\n",
        "    packing=False,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16, # Note: you can change this depending on the amount of VRAM your GPU has\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_checkpointing=False,\n",
        "    optim=\"adamw_torch_fused\", # Note: if you try \"adamw\", you will get an error\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=BASE_LEARNING_RATE,\n",
        "    fp16=True if torch_dtype == torch.float16 else False,\n",
        "    bf16=True if torch_dtype == torch.float16 else False,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    push_to_hub=False,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "# There are a lot of settings in the sft_config, so feel free to uncomment this and inspect it if you want\n",
        "#sft_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9f2984-aa4a-4ebc-8d2c-bf6d7f028608",
      "metadata": {
        "id": "4f9f2984-aa4a-4ebc-8d2c-bf6d7f028608"
      },
      "source": [
        "Config setup, now we can train our model with `SFTTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a74f51",
      "metadata": {
        "id": "a6a74f51"
      },
      "outputs": [],
      "source": [
        "# Supervised Fine-Tuning = provide input and desired output samples\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Create Trainer object\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbb9fd0-e0ed-4c18-bfb4-a1ad2e40debd",
      "metadata": {
        "id": "cfbb9fd0-e0ed-4c18-bfb4-a1ad2e40debd"
      },
      "source": [
        "Woohoo! Looks like our training accuracy went up.\n",
        "\n",
        "Let's inspect the loss curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e176da4",
      "metadata": {
        "id": "0e176da4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Access the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract training / validation loss\n",
        "train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
        "epoch_train = [log[\"epoch\"] for log in log_history if \"loss\" in log]\n",
        "eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
        "epoch_eval = [log[\"epoch\"] for log in log_history if \"eval_loss\" in log]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(epoch_train, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epoch_eval, eval_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9d8bbd",
      "metadata": {
        "id": "ab9d8bbd"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8abb4c35",
      "metadata": {
        "id": "8abb4c35"
      },
      "outputs": [],
      "source": [
        "# Remove all the checkpoint folders (since we've already saved the best model)\n",
        "!rm -rf ./checkpoint_models/checkpoint-*/*\n",
        "!rm -rf ./checkpoint_models/checkpoint-*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b744778",
      "metadata": {
        "id": "1b744778"
      },
      "source": [
        "## Load the trained model back in and see how it performs\n",
        "\n",
        "We've now fine-tuned our own Gemma 3 270M to do a specific task, let's load it back in and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d5d374",
      "metadata": {
        "id": "d2d5d374"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ebfeae",
      "metadata": {
        "id": "c2ebfeae"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and see how it goes\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load trained model\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=CHECKPOINT_DIR_NAME,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739c608d",
      "metadata": {
        "id": "739c608d"
      },
      "outputs": [],
      "source": [
        "loaded_model_pipeline = pipeline(\"text-generation\",\n",
        "                                 model=loaded_model,\n",
        "                                 tokenizer=tokenizer)\n",
        "\n",
        "loaded_model_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db929ad",
      "metadata": {
        "id": "8db929ad"
      },
      "outputs": [],
      "source": [
        "dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa55181-5aba-47f4-b237-da7fcb57bfd5",
      "metadata": {
        "id": "5fa55181-5aba-47f4-b237-da7fcb57bfd5"
      },
      "source": [
        "Let's now perform inference with our fine-tuned model on a random sample from the test dataset (our model has never seen these samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee88d7fa",
      "metadata": {
        "id": "ee88d7fa"
      },
      "outputs": [],
      "source": [
        "# Get a random sample\n",
        "random_test_idx = get_random_idx(dataset[\"test\"])\n",
        "random_test_sample = dataset[\"test\"][random_test_idx]\n",
        "\n",
        "# Apply the chat template\n",
        "input_prompt = pipe.tokenizer.apply_chat_template(conversation=random_test_sample[\"messages\"][:1],\n",
        "                                                  tokenize=False,\n",
        "                                                  add_generation_prompt=True)\n",
        "\n",
        "# Let's run the default model on our input\n",
        "default_outputs = loaded_model_pipeline(text_inputs=input_prompt,\n",
        "                                        max_new_tokens=256)\n",
        "\n",
        "# View and compare the outputs\n",
        "print(f\"[INFO] Input:\\n{input_prompt}\\n\")\n",
        "print(f\"[INFO] Output:\\n{default_outputs[0]['generated_text'][len(input_prompt):]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f424585",
      "metadata": {
        "id": "1f424585"
      },
      "outputs": [],
      "source": [
        "print(random_test_sample[\"gpt-oss-120b-label-condensed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2efdc312-31c9-45cb-93f3-c9844d832b31",
      "metadata": {
        "id": "2efdc312-31c9-45cb-93f3-c9844d832b31"
      },
      "source": [
        "## Counting the number of parameters in our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c4e8ce",
      "metadata": {
        "id": "56c4e8ce"
      },
      "outputs": [],
      "source": [
        "def get_model_num_params(model):\n",
        "    \"\"\"\n",
        "    Returns the number of trainable, non-trainable and total parameters of a PyTorch model.\n",
        "    \"\"\"\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "    total_params = trainable_params + non_trainable_params\n",
        "    return {\"trainable_params\": trainable_params,\n",
        "            \"non_trainable_params\": non_trainable_params,\n",
        "            \"total_params\": total_params}\n",
        "\n",
        "# Get parameters of our fine-tuned model\n",
        "model_params = get_model_num_params(loaded_model)\n",
        "print(f\"Trainable parameters: {model_params['trainable_params']:,}\")\n",
        "print(f\"Non-trainable parameters: {model_params['non_trainable_params']:,}\")\n",
        "print(f\"Total parameters: {model_params['total_params']:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c365fe",
      "metadata": {
        "id": "10c365fe"
      },
      "outputs": [],
      "source": [
        "# Our model is 270M parameters, GPT-OSS-120B is 120B parameters\n",
        "120_000_000_000 / 270_000_000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7091522a-816b-48c2-b15b-2b163471fa00",
      "metadata": {
        "id": "7091522a-816b-48c2-b15b-2b163471fa00"
      },
      "source": [
        "By fine-tuning Gemma 3 270M we distill the capabilities of a 120B parameter model into a model 444x smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c61ef3",
      "metadata": {
        "id": "79c61ef3"
      },
      "source": [
        "## Uploading our fine-tuned model to the Hugging Face Hub\n",
        "\n",
        "We can upload our fine-tuned model to the Hugging Face Hub so other people can use it and test it out.\n",
        "\n",
        "First, let's load it in ourselves and confirm it works how we'd like it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60366c1e",
      "metadata": {
        "id": "60366c1e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "MODEL_PATH = \"./checkpoint_models/\"\n",
        "\n",
        "# Load the model into a pipeline\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_PATH,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_PATH\n",
        ")\n",
        "\n",
        "# Create a pipeline from the loaded model\n",
        "loaded_model_pipeline = pipeline(\"text-generation\",\n",
        "                                 model=loaded_model,\n",
        "                                 tokenizer=tokenizer)\n",
        "\n",
        "# Test the loaded model on raw text (this won't work as well as formatted text)\n",
        "test_input_message = \"Hello my name is Daniel!\"\n",
        "loaded_model_pipeline(test_input_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae54a14a-cdf2-49f5-8a43-7fd555a5642c",
      "metadata": {
        "id": "ae54a14a-cdf2-49f5-8a43-7fd555a5642c"
      },
      "source": [
        "Let's create a helper function to format our input text into message format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4546c352-9764-4cff-b1e9-3db235a3b1b1",
      "metadata": {
        "id": "4546c352-9764-4cff-b1e9-3db235a3b1b1"
      },
      "outputs": [],
      "source": [
        "def format_message(input):\n",
        "    return [{\"role\": \"user\", \"content\": input}]\n",
        "\n",
        "input_formatted = format_message(input=test_input_message)\n",
        "input_formatted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6251b6-09b1-44e2-a419-58e2b315a83d",
      "metadata": {
        "id": "cf6251b6-09b1-44e2-a419-58e2b315a83d"
      },
      "source": [
        "Now we can turn it into a prompt with our `tokenizer` and the `apply_chat_template` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7686504-0d33-4751-a8b4-55107719ef9d",
      "metadata": {
        "id": "c7686504-0d33-4751-a8b4-55107719ef9d"
      },
      "outputs": [],
      "source": [
        "input_prompt = loaded_model_pipeline.tokenizer.apply_chat_template(conversation=input_formatted,\n",
        "                                                                   tokenize=False,\n",
        "                                                                   add_generation_prompt=True)\n",
        "\n",
        "input_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "586b36c3-9677-408c-a8a9-5925e5043d71",
      "metadata": {
        "id": "586b36c3-9677-408c-a8a9-5925e5043d71"
      },
      "source": [
        "Beautiful! Time to run our fine-tuned model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41583a8d-13a8-4d57-b7d4-99f7c6619843",
      "metadata": {
        "id": "41583a8d-13a8-4d57-b7d4-99f7c6619843"
      },
      "outputs": [],
      "source": [
        "loaded_model_outputs = loaded_model_pipeline(text_inputs=input_prompt,\n",
        "                                             max_new_tokens=256)\n",
        "\n",
        "# View and compare the outputs\n",
        "print(f\"[INFO] Input:\\n{input_prompt}\\n\")\n",
        "print(f\"[INFO] Output:\\n{loaded_model_outputs[0]['generated_text'][len(input_prompt):]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a05457-44f7-4a1b-8e4c-2ee6b49ff42a",
      "metadata": {
        "id": "83a05457-44f7-4a1b-8e4c-2ee6b49ff42a"
      },
      "source": [
        "Okay let's make another helper function to predict on any given sample input.\n",
        "\n",
        "We'll also return the inference time of our model so we can see how long things take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2341bd46-23d7-424a-b59d-d1d52481890b",
      "metadata": {
        "id": "2341bd46-23d7-424a-b59d-d1d52481890b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def pred_on_text(input_text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    raw_output = loaded_model_pipeline(text_inputs=[{\"role\": \"user\",\n",
        "                                                    \"content\": input_text}],\n",
        "                                       max_new_tokens=256,\n",
        "                                       disable_compile=True)\n",
        "    end_time = time.time()\n",
        "    total_time = round(end_time - start_time, 4)\n",
        "\n",
        "    generated_text = raw_output[0][\"generated_text\"][1][\"content\"]\n",
        "\n",
        "    return generated_text, raw_output, total_time\n",
        "\n",
        "pred_on_text(input_text=\"British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast and fried tomatoes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae3716aa-7eea-433d-b002-d3f9b1270784",
      "metadata": {
        "id": "ae3716aa-7eea-433d-b002-d3f9b1270784"
      },
      "source": [
        "Nice! Looks like our model is working well enough (of course we could always improve it over time with more testing and different samples).\n",
        "\n",
        "Let's upload it to the Hugging Face Hub.\n",
        "\n",
        "We can do so using the [`huggingface_hub` library](https://huggingface.co/docs/huggingface_hub/en/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b536ac2f-fd95-441f-a708-b1b685e07e33",
      "metadata": {
        "id": "b536ac2f-fd95-441f-a708-b1b685e07e33"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Give our model a name (this is in the format [Hugging Face Username]/[Target Model Name]\n",
        "repo_id = \"mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1\"\n",
        "\n",
        "# Create the repo\n",
        "create_repo(repo_id,\n",
        "            repo_type=\"model\",\n",
        "            exist_ok=True)\n",
        "\n",
        "# Upload the entire model folder containing our model files\n",
        "api.upload_folder(\n",
        "    folder_path=\"./checkpoint_models/\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2b2f73f-ba69-472c-b08b-9ec5ee360508",
      "metadata": {
        "id": "a2b2f73f-ba69-472c-b08b-9ec5ee360508"
      },
      "source": [
        "Woohoo! Our model is officially on the Hugging Face Hub.\n",
        "\n",
        "Now not only can we redownload it and use it again, others can download it and use it for themselves (of course you can make the model private if you like too)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afeae0a6-4384-41ea-a2e7-88ec31fdeddf",
      "metadata": {
        "id": "afeae0a6-4384-41ea-a2e7-88ec31fdeddf"
      },
      "source": [
        "## Turn our model into a demo\n",
        "\n",
        "Right now our model seems to be working quite well for our specific use case.\n",
        "\n",
        "However, it takes some coding to be able to use it.\n",
        "\n",
        "What if we wanted to allow someone who wasn't familiar with programming to try it out?\n",
        "\n",
        "To do so, we can turn our model into a [Gradio](https://www.gradio.app) demo and upload it to [Hugging Face Spaces](https://huggingface.co/spaces) (a place to share all kinds of small applications).\n",
        "\n",
        "Gradio allows us to turn our model into an easy to use and sharable demo anyone can try.\n",
        "\n",
        "Gradio demos work on the premise of: input (text) -> function (our model) -> output (text)\n",
        "\n",
        "We've already go a function ready with `pred_on_text` so we can wrap this with some Gradio code.\n",
        "\n",
        "To create a sharable demo, we'll need the following files:\n",
        "\n",
        "* `app.py` - Entry point for our app, all of our application code will go in here.\n",
        "* `README.md` - Tells people what our app does.\n",
        "    * **Note:** Hugging Face Spaces use a special \"front matter\" (text at the start of a `README.md` file) to add various attributes to a Hugging Face Space, we'll see this below.\n",
        "* `requirements.txt` - Tells Hugging Face Spaces what our app requires.\n",
        "    * `torch`, `transformers`, `gradio`, `accelerate`\n",
        "\n",
        "Let's make a folder to store our demo application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4f64de-af13-47c7-ba5f-9b176bb58880",
      "metadata": {
        "id": "fa4f64de-af13-47c7-ba5f-9b176bb58880"
      },
      "outputs": [],
      "source": [
        "!mkdir demos/\n",
        "!mkdir demos/FoodExtract"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19ae722-2c37-4c53-a0c3-6dc484ba39a9",
      "metadata": {
        "id": "e19ae722-2c37-4c53-a0c3-6dc484ba39a9"
      },
      "source": [
        "### Creating the `app.py` file\n",
        "\n",
        "When running our app on Hugging Face Spaces, we have the option to run our model on a GPU thanks to Hugging Face's [ZeroGPU](https://huggingface.co/docs/hub/en/spaces-zerogpu) feature.\n",
        "\n",
        "This is optional, however, it's highly recommend you run a model such as Gemma 3 270M on the GPU as we'll see significant speedups.\n",
        "\n",
        "You can run a function on a GPU by importing `spaces` and then using the `@spaces.GPU` decorator on your target function.\n",
        "\n",
        "```python\n",
        "import spaces\n",
        "\n",
        "@spaces.GPU\n",
        "def function_to_run_on_the_gpu():\n",
        "    pass\n",
        "```\n",
        "\n",
        "To ensure your model runs on the GPU, be sure to select a ZeroGPU instance in your Hugging Face Space settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26b9719a-a52f-4a06-a5e7-e5e25c0f05cd",
      "metadata": {
        "id": "26b9719a-a52f-4a06-a5e7-e5e25c0f05cd"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/FoodExtract/app.py\n",
        "\n",
        "# Load dependencies\n",
        "import time\n",
        "import transformers\n",
        "import torch\n",
        "import spaces # Optional: run our model on the GPU (this will be much faster inference)\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "@spaces.GPU # Optional: run our model on the GPU (this will be much faster inference)\n",
        "def pred_on_text(input_text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    raw_output = loaded_model_pipeline(text_inputs=[{\"role\": \"user\",\n",
        "                                                    \"content\": input_text}],\n",
        "                                       max_new_tokens=256,\n",
        "                                       disable_compile=True)\n",
        "    end_time = time.time()\n",
        "    total_time = round(end_time - start_time, 4)\n",
        "\n",
        "    generated_text = raw_output[0][\"generated_text\"][1][\"content\"]\n",
        "\n",
        "    return generated_text, raw_output, total_time\n",
        "\n",
        "# Load the model (from our Hugging Face Repo)\n",
        "# Note: You may have to replace my username `mrdbourke` for your own\n",
        "MODEL_PATH = \"mrdbourke/FoodExtract-gemma-3-270m-fine-tune-v1\"\n",
        "\n",
        "# Load the model into a pipeline\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_PATH,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_PATH\n",
        ")\n",
        "\n",
        "# Create model pipeline\n",
        "loaded_model_pipeline = pipeline(\"text-generation\",\n",
        "                                 model=loaded_model,\n",
        "                                 tokenizer=tokenizer)\n",
        "\n",
        "# Create the demo\n",
        "description = \"\"\"Extract food and drink items from text with a fine-tuned SLM (Small Language Model) or more specifically a fine-tuned [Gemma 3 270M](https://huggingface.co/google/gemma-3-270m-it).\n",
        "\n",
        "Our model has been fine-tuned on the [FoodExtract-1k dataset](https://huggingface.co/datasets/mrdbourke/FoodExtract-1k).\n",
        "\n",
        "* Input (str): Raw text strings or image captions (e.g. \"A photo of a dog sitting on a beach\" or \"A breakfast plate with bacon, eggs and toast\")\n",
        "* Output (str): Generated text with food/not_food classification as well as noun extracted food and drink items and various food tags.\n",
        "\n",
        "For example:\n",
        "\n",
        "* Input: \"For breakfast I had eggs, bacon and toast and a glass of orange juice\"\n",
        "* Output:\n",
        "\n",
        "```\n",
        "food_or_drink: 1\n",
        "tags: fi, di\n",
        "foods: eggs, bacon, toast\n",
        "drinks: orange juice\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "demo = gr.Interface(fn=pred_on_text,\n",
        "                    inputs=gr.TextArea(lines=4, label=\"Input Text\"),\n",
        "                    outputs=[gr.TextArea(lines=4, label=\"Generated Text\"),\n",
        "                             gr.TextArea(lines=7, label=\"Raw Output\"),\n",
        "                             gr.Number(label=\"Generation Time (s)\")],\n",
        "                    title=\" Structured FoodExtract with a Fine-Tuned Gemma 3 270M\",\n",
        "                    description=description,\n",
        "                    examples=[[\"Hello world! This is my first fine-tuned LLM!\"],\n",
        "                              [\"A plate of food with grilled barramundi, salad with avocado, olives, tomatoes and Italian dressing\"],\n",
        "                              [\"British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast and fried tomatoes\"],\n",
        "                              [\"Steak tacos\"],\n",
        "                              [\"A photo of a dog sitting on a beach\"]]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d050ad82-e40c-4224-9f2e-0ed7e431a5b0",
      "metadata": {
        "id": "d050ad82-e40c-4224-9f2e-0ed7e431a5b0"
      },
      "source": [
        "### Create the `README.md` file\n",
        "\n",
        "The `README.md` file will tell people what our app does.\n",
        "\n",
        "We could add more information here if we wanted to but for now we'll keep it simple.\n",
        "\n",
        "Notice the special text at the top of the file below (the text between the `---`), these are some settings for the Space, you can see the [settings for these in the docs](https://huggingface.co/docs/hub/en/spaces-config-reference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cb083d-8258-4a5d-a54e-52d22c6a5789",
      "metadata": {
        "id": "33cb083d-8258-4a5d-a54e-52d22c6a5789"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/FoodExtract/README.md\n",
        "---\n",
        "title: FoodExtract Fine-tuned LLM Structued Data Extractor\n",
        "emoji: \n",
        "colorFrom: green\n",
        "colorTo: blue\n",
        "sdk: gradio\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "Fine-tuned Gemma 3 270M to extract food and drink items from raw text.\n",
        "\n",
        "Input can be any form of real text and output will be a formatted string such as the following:\n",
        "\n",
        "```\n",
        "food_or_drink: 1\n",
        "tags: fi, re\n",
        "foods: tacos, milk, red apple, pineapple, cherries, fried chicken, steak, mayonnaise\n",
        "drinks: iced latte, matcha latte\n",
        "```\n",
        "\n",
        "The tags map to the following items:\n",
        "\n",
        "```\n",
        "tags_dict = {'np': 'nutrition_panel',\n",
        " 'il': 'ingredient list',\n",
        " 'me': 'menu',\n",
        " 're': 'recipe',\n",
        " 'fi': 'food_items',\n",
        " 'di': 'drink_items',\n",
        " 'fa': 'food_advertistment',\n",
        " 'fp': 'food_packaging'}\n",
        "```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f799cba2-a825-467f-aa20-734bb4bde1a3",
      "metadata": {
        "id": "f799cba2-a825-467f-aa20-734bb4bde1a3"
      },
      "source": [
        "### Create the `requirements.txt` file\n",
        "\n",
        "This will tell the Hugging Face Space what libraries we'd like it to run inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e17c94-07bf-4c40-a045-f64f58256a0a",
      "metadata": {
        "id": "d7e17c94-07bf-4c40-a045-f64f58256a0a"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/FoodExtract/requirements.txt\n",
        "transformers\n",
        "gradio\n",
        "torch\n",
        "accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a688a8d9-a0c6-42f8-a621-4ed847cd25c9",
      "metadata": {
        "id": "a688a8d9-a0c6-42f8-a621-4ed847cd25c9"
      },
      "source": [
        "### Upload our demo to the Hugging Face Hub\n",
        "\n",
        "We can upload our demo to the Hugging Face Hub in a similar way to uploading our model.\n",
        "\n",
        "We could also upload it file by file via the Hugging Face Spaces interface.\n",
        "\n",
        "But let's stick to the code-first approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4988447-8a2a-40da-bffe-4636ab960b9c",
      "metadata": {
        "id": "a4988447-8a2a-40da-bffe-4636ab960b9c"
      },
      "outputs": [],
      "source": [
        "# 1. Import the required methods for uploading to the Hugging Face Hub\n",
        "from huggingface_hub import (\n",
        "    create_repo,\n",
        "    get_full_repo_name,\n",
        "    upload_file, # for uploading a single file (if necessary)\n",
        "    upload_folder # for uploading multiple files (in a folder)\n",
        ")\n",
        "\n",
        "# 2. Define the parameters we'd like to use for the upload\n",
        "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/FoodExtract/\"\n",
        "HF_TARGET_SPACE_NAME = \"FoodExtract-v1\"\n",
        "HF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\n",
        "HF_SPACE_SDK = \"gradio\"\n",
        "HF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n",
        "\n",
        "# 3. Create a Space repository on Hugging Face Hub\n",
        "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    private=False, # set to True if you don't want your Space to be accessible to others\n",
        "    space_sdk=HF_SPACE_SDK,\n",
        "    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists\n",
        ")\n",
        "\n",
        "# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\n",
        "full_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
        "print(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n",
        "\n",
        "# 5. Upload our demo folder\n",
        "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id=full_hf_repo_name,\n",
        "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
        "    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n",
        "    # token=HF_TOKEN, # optional: set token manually\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message=\"Uploading FoodExtract demo app.py\"\n",
        ")\n",
        "print(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1248ec-8e21-46a5-8789-004b5850dece",
      "metadata": {
        "id": "6f1248ec-8e21-46a5-8789-004b5850dece"
      },
      "source": [
        "Nice!\n",
        "\n",
        "It looks like our demo upload worked!\n",
        "\n",
        "We can try it out via the URL (in my case, it's: [https://huggingface.co/spaces/mrdbourke/FoodExtract-v1](https://huggingface.co/spaces/mrdbourke/FoodExtract-v1)). And we can even embed it right in our notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dc1a559-079a-49cd-9197-2b1b06f64c5b",
      "metadata": {
        "id": "5dc1a559-079a-49cd-9197-2b1b06f64c5b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "html_code = \"\"\"<iframe\n",
        "\tsrc=\"https://mrdbourke-foodextract-v1.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"1000\"\n",
        "></iframe>\"\"\"\n",
        "\n",
        "display(HTML(html_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf507343-503e-4785-93a9-61a0b9fd779f",
      "metadata": {
        "id": "bf507343-503e-4785-93a9-61a0b9fd779f"
      },
      "source": [
        "How cool!\n",
        "\n",
        "We've now got a sharable demo of a fine-tuned LLM which anyone can try out for themselves."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6216a5-db4d-4220-a18a-f378b4490a9d",
      "metadata": {
        "id": "bc6216a5-db4d-4220-a18a-f378b4490a9d"
      },
      "source": [
        "## Bonus: Speeding up our model with batched inference\n",
        "\n",
        "Right now our model only inferences on one sample at a time but as is the case with many machine learning models, we could perform inference on multiple samples (also referred to as a batch) to significantly improve throughout.\n",
        "\n",
        "In batched inference mode, your model performs predictions on X number of samples at once, this can dramatically improve sample throughput.\n",
        "\n",
        "The number of samples you can predict on at once will depend on a few factors:\n",
        "\n",
        "* The size of your model (e.g. if your model is quite large, it may only be able to predict on 1 sample at time)\n",
        "* The size of your compute VRAM (e.g. if your compute VRAM is already saturated, add multiple samples at a time may result in errors)\n",
        "* The size of your samples (if one of your samples is 100x the size of others, this may cause errors with batched inference)\n",
        "\n",
        "To find an optimal batch size for our setup, we can run an experiment:\n",
        "\n",
        "* Loop through different batch sizes and measure the throughput for each batch size.\n",
        "    * Why do we do this?\n",
        "        * It's hard to tell the ideal batch size ahead of time.\n",
        "        * So we experiment from say 1, 2, 4, 8, 16, 32, 64 batch sizes and see which performs best.\n",
        "        * Just because we may get a speed up from using batch size 8, doesn't mean 64 will be better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74535c69-d30a-4d95-aa0d-b477ac0d6f2c",
      "metadata": {
        "id": "74535c69-d30a-4d95-aa0d-b477ac0d6f2c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"mrdbourke/FoodExtract-1k\")\n",
        "\n",
        "print(f\"[INFO] Number of samples in the dataset: {len(dataset['train'])}\")\n",
        "\n",
        "def sample_to_conversation(sample):\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": sample[\"sequence\"]}, # Load the sequence from the dataset\n",
        "            {\"role\": \"system\", \"content\": sample[\"gpt-oss-120b-label-condensed\"]} # Load the gpt-oss-120b generated label\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Map our sample_to_conversation function to dataset\n",
        "dataset = dataset.map(sample_to_conversation,\n",
        "                      batched=False)\n",
        "\n",
        "# Create a train/test split\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2,\n",
        "                                            shuffle=False,\n",
        "                                            seed=42)\n",
        "\n",
        "# Number #1 rule in machine learning\n",
        "# Always train on the train set and test on the test set\n",
        "# This gives us an indication of how our model will perform in the real world\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33695408-65b3-4708-a008-30acd385f46c",
      "metadata": {
        "id": "33695408-65b3-4708-a008-30acd385f46c"
      },
      "outputs": [],
      "source": [
        "# Step 1: Need to turn our samples into batches (e.g. lists of samples)\n",
        "print(f\"[INFO] Formatting test samples into list prompts...\")\n",
        "test_input_prompts = [\n",
        "    loaded_model_pipeline.tokenizer.apply_chat_template(\n",
        "        item[\"messages\"][:1],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    for item in dataset[\"test\"]\n",
        "]\n",
        "print(f\"[INFO] Number of test sample prompts: {len(test_input_prompts)}\")\n",
        "test_input_prompts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b3e66c-a5ba-4056-bd9a-97cc9aab7f74",
      "metadata": {
        "id": "63b3e66c-a5ba-4056-bd9a-97cc9aab7f74"
      },
      "outputs": [],
      "source": [
        "# Step 2: Need to perform batched inference and time each step\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "# Let's write a list of batch sizes to test\n",
        "chunk_sizes_to_test = [1, 4, 8, 16, 32, 64, 128]\n",
        "timing_dict = {}\n",
        "\n",
        "# Loop through each batch size and time the inference\n",
        "for CHUNK_SIZE in chunk_sizes_to_test:\n",
        "    print(f\"[INFO] Making predictions with batch size: {CHUNK_SIZE}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for chunk_number in tqdm(range(round(len(test_input_prompts) / CHUNK_SIZE))):\n",
        "        batched_inputs = test_input_prompts[(CHUNK_SIZE * chunk_number): CHUNK_SIZE * (chunk_number + 1)]\n",
        "        batched_outputs = loaded_model_pipeline(text_inputs=batched_inputs,\n",
        "                                                batch_size=CHUNK_SIZE,\n",
        "                                                max_new_tokens=256,\n",
        "                                                disable_compile=True)\n",
        "\n",
        "        all_outputs += batched_outputs\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    timing_dict[CHUNK_SIZE] = total_time\n",
        "    print()\n",
        "    print(f\"[INFO] Total time for batch size {CHUNK_SIZE}: {total_time:.2f}s\")\n",
        "    print(\"=\"*80 + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cf064ff-a84e-4d01-b377-60c0a62813e5",
      "metadata": {
        "id": "6cf064ff-a84e-4d01-b377-60c0a62813e5"
      },
      "source": [
        "Batched inference complete! Let's make a plot comparing different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29098f5-13ff-47ee-b1bd-13ec50aa914f",
      "metadata": {
        "id": "b29098f5-13ff-47ee-b1bd-13ec50aa914f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "data = timing_dict\n",
        "\n",
        "total_samples = len(dataset[\"test\"])\n",
        "\n",
        "batch_sizes = list(data.keys())\n",
        "inference_times = list(data.values())\n",
        "samples_per_second = [total_samples / time for bs, time in data.items()]\n",
        "\n",
        "# Create side-by-side plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# --- Left plot: Total Inference Time ---\n",
        "ax1.bar([str(bs) for bs in batch_sizes], inference_times, color='steelblue')\n",
        "ax1.set_xlabel('Batch Size')\n",
        "ax1.set_ylabel('Total Inference Time (s)')\n",
        "ax1.set_title('Inference Time by Batch Size')\n",
        "\n",
        "for i, v in enumerate(inference_times):\n",
        "    ax1.text(i, v + 1, f'{v:.1f}', ha='center', fontsize=9)\n",
        "\n",
        "# --- ARROW LOGIC (Left) ---\n",
        "# 1. Identify Start (Slowest) and End (Fastest)\n",
        "start_val = max(inference_times)\n",
        "end_val = min(inference_times)\n",
        "start_idx = inference_times.index(start_val)\n",
        "end_idx = inference_times.index(end_val)\n",
        "\n",
        "speedup = start_val / end_val\n",
        "\n",
        "# 2. Draw Arrow (No Text)\n",
        "# connectionstyle \"rad=-0.3\" arcs the arrow upwards\n",
        "ax1.annotate(\"\",\n",
        "             xy=(end_idx, end_val+(0.5*end_val)),\n",
        "             xytext=(start_idx+0.25, start_val+10),\n",
        "             arrowprops=dict(arrowstyle=\"->\", color='green', lw=1.5, connectionstyle=\"arc3,rad=-0.3\"))\n",
        "\n",
        "# 3. Place Text at Midpoint\n",
        "mid_x = (start_idx + end_idx) / 2\n",
        "# Place text slightly above the highest point of the two bars\n",
        "text_y = max(start_val, end_val) + (max(inference_times) * 0.1)\n",
        "\n",
        "ax1.text(mid_x+0.5, text_y-150, f\"{speedup:.1f}x speedup\",\n",
        "         ha='center', va='bottom', fontweight='bold',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"none\", alpha=0.8))\n",
        "\n",
        "ax1.set_ylim(0, max(inference_times) * 1.35) # Increase headroom for text\n",
        "\n",
        "\n",
        "# --- Right plot: Samples per Second ---\n",
        "ax2.bar([str(bs) for bs in batch_sizes], samples_per_second, color='coral')\n",
        "ax2.set_xlabel('Batch Size')\n",
        "ax2.set_ylabel('Samples per Second')\n",
        "ax2.set_title('Throughput by Batch Size')\n",
        "\n",
        "for i, v in enumerate(samples_per_second):\n",
        "    ax2.text(i, v + 0.05, f'{v:.2f}', ha='center', fontsize=9)\n",
        "\n",
        "# --- ARROW LOGIC (Right) ---\n",
        "# 1. Identify Start (Slowest) and End (Fastest)\n",
        "start_val_t = min(samples_per_second)\n",
        "end_val_t = max(samples_per_second)\n",
        "start_idx_t = samples_per_second.index(start_val_t)\n",
        "end_idx_t = samples_per_second.index(end_val_t)\n",
        "\n",
        "speedup_t = end_val_t / start_val_t\n",
        "\n",
        "# 2. Draw Arrow (No Text)\n",
        "ax2.annotate(\"\",\n",
        "             xy=(end_idx_t-(0.05*end_idx_t), end_val_t+(0.025*end_val_t)),\n",
        "             xytext=(start_idx_t, start_val_t+0.6),\n",
        "             arrowprops=dict(arrowstyle=\"->\", color='green', lw=1.5, connectionstyle=\"arc3,rad=-0.3\"))\n",
        "\n",
        "# 3. Place Text at Midpoint\n",
        "mid_x_t = (start_idx_t + end_idx_t) / 2\n",
        "text_y_t = max(start_val_t, end_val_t) + (max(samples_per_second) * 0.1)\n",
        "\n",
        "ax2.text(mid_x_t-0.5, text_y_t-4.5, f\"{speedup_t:.1f}x speedup\",\n",
        "         ha='center', va='bottom', fontweight='bold',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"none\", alpha=0.8))\n",
        "\n",
        "ax2.set_ylim(0, max(samples_per_second) * 1.35) # Increase headroom\n",
        "\n",
        "plt.suptitle(\"Inference with Fine-Tuned Gemma 3 270M on NVIDIA DGX Spark\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('inference_benchmark.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418e27ab",
      "metadata": {
        "id": "418e27ab"
      },
      "source": [
        "We get a 4-6x speedup when using batches!\n",
        "\n",
        "At 10 samples per second, that means we can inference on ~800k samples in a day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6e07e9",
      "metadata": {
        "id": "7a6e07e9"
      },
      "outputs": [],
      "source": [
        "samples_per_second = round(len(dataset[\"test\"]) / min(timing_dict.values()), 2)\n",
        "seconds_in_a_day = 86_400\n",
        "samples_per_day = seconds_in_a_day * samples_per_second\n",
        "\n",
        "print(f\"[INFO] Number of samples per second: {samples_per_second} | Number of samples per day: {samples_per_day}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0991404-74ac-4dc4-b80d-d04a883b0b94",
      "metadata": {
        "id": "e0991404-74ac-4dc4-b80d-d04a883b0b94"
      },
      "source": [
        "## Bonus: Performing evaluations on our model\n",
        "\n",
        "We can evaluate our model directly against the original labels from `gpt-oss-120b` and see how it stacks up.\n",
        "\n",
        "For example, we could compare the following:\n",
        "\n",
        "* F1 Score\n",
        "* Precision\n",
        "* Recall\n",
        "* Pure accuracy\n",
        "\n",
        "Have these metrics as well as samples which are different to the ground truth would allow us to further explore where our model needs improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe093b65",
      "metadata": {
        "id": "fe093b65"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "There are several avenues we could approach next.\n",
        "\n",
        "1. Improve the data sampling - if our model makes mistakes, could we improve the input data? For example, more samples on certain tasks where the model is weaker.\n",
        "\n",
        "2. Quantisation - right now our model is ~500MB, could we make this smaller with the help of INT8 quantisation? (this could also help speed up inference time)\n",
        "\n",
        "3. Output compression - our output is compressed to a simple YAML-like format, but is there a better compression option to generate even less tokens?\n",
        "\n",
        "4. Test the model on a large unseen sample - practice using the model on a large corpus of image captions (e.g. 10,000+) and randomly inspect them to find failure cases to boost the model performance\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}